{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PID Classification with Neural Networks (Unsupervised Training)\n",
    "\n",
    "This example illustrates the classification of particle types using [tensorflow](https://www.tensorflow.org/)/[neupy](http://neupy.com/pages/home.html) neural networks. The unsupervised training uses a Growing Neural Gas (GNG) model with MC generated data of [BaBar](https://www.flickr.com/photos/slaclab/46211844232). As the training might take a very long time, it is possible to persist a network and read the trained network for data analytics.\n",
    "\n",
    "We begin with the standard imports. This notebook uses numpy, pandas, seaborn, matplotlib, and tensorflow/keras. The data are read from a ROOT file with uproot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import uproot\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset\n",
    "We are reading the training dataset from a ROOT file. The file contains particle momentum (mom) and track elevation (theta), dE/dx measurement from silicon vertex tracker (svt) and drift chamber (dch), as well as energy deposit in the electromagnetic calorimeter (emc), the cerenkov angle in the DIRC (drc), and the hit patterns in the instrumented flux return (ifr). The file in addition holds higher level features like partial energy sums, zernicke momenta, likelihood etc. \n",
    "\n",
    "The particles are labeled (id)\n",
    "* Electron = 0\n",
    "* Muon = 1\n",
    "* Pion = 2\n",
    "* Kaon = 3\n",
    "* Proton = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Electron', 'Muon', 'Pion', 'Kaon', 'Proton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.open(\"pid.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = file[\"PidTuple\"]\n",
    "tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.numentries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assemble the training dataset as a pandas dataframe and plot the particle statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tree.arrays([\"id\", \"mom\", \"theta\", \"svt\", \"emc\", \"drc\", \"dch\", \"ifr\"])\n",
    "dataset = pd.DataFrame(data)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electron = dataset[dataset[b'id']==0][b'id'].value_counts()\n",
    "muon = dataset[dataset[b'id']==1][b'id'].value_counts()\n",
    "pion = dataset[dataset[b'id']==2][b'id'].value_counts()\n",
    "kaon = dataset[dataset[b'id']==3][b'id'].value_counts()\n",
    "proton = dataset[dataset[b'id']==4][b'id'].value_counts()\n",
    "\n",
    "df = pd.DataFrame([electron, muon, pion, kaon, proton])\n",
    "df.index = target_names\n",
    "df.plot(kind='bar',stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "We generate a training dataset (90%) and a test dataset (10%) from the input dataset. The vectors are shuffled in random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.9,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a correlation matrix of the feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_dataset[[b'mom', b'theta', b'svt', b'emc', b'drc', b'dch', b'ifr', b'id']]\n",
    "train[b'id'] = train[b'id'].map( {0: 'electron', 1: 'muon', 2: 'pion', 3:'kaon', 4:'proton'} ).astype(str)\n",
    "sns.pairplot(data=train[:1000], hue=b'id', diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the particle labels to be used for particle tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset.pop(b'id')\n",
    "test_labels = test_dataset.pop(b'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the training and test vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build and train the GNG Model\n",
    "The keras model implements a neural network with two hidden layers. The input feature vector represents measured detector quantities as defined above. The output reflects the particle probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neupy import algorithms, utils\n",
    "def build_model(dimension, max_nodes=10000, step=0.5, n_start_nodes=2, max_edge_age=15):\n",
    "    model = algorithms.GrowingNeuralGas(\n",
    "        n_inputs=dimension,\n",
    "        n_start_nodes=n_start_nodes,\n",
    "\n",
    "        shuffle_data=True,\n",
    "        verbose=True,\n",
    "\n",
    "        step=step,\n",
    "        neighbour_step=0.05,\n",
    "\n",
    "        max_edge_age=max_edge_age,\n",
    "        max_nodes=max_nodes,\n",
    "\n",
    "        n_iter_before_neuron_added=10,\n",
    "        after_split_error_decay_rate=0.1,\n",
    "        error_decay_rate=0.995,\n",
    "        min_distance_for_update=0.01,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(len(train_dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the GNG network (This might take a _very_ long time, depending on your CPU power). Draw the GNG network for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.reproducible()\n",
    "for epoch in range(10):\n",
    "    model.train(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the GNG network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('gng.dill', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze the GNG Network\n",
    "We identify the clusters in the network that represent the particle types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('gng.dill', 'rb') as f:\n",
    "    model = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a k-means algorithm to identify clusters of nodes. We use 5 categories as we have 5 particle types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train = train_dataset.to_numpy()\n",
    "nodes = model.graph.nodes\n",
    "weights = np.concatenate([node.weight for node in nodes])\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(weights)\n",
    "kpred_labels = kmeans.predict(train)\n",
    "kpredictions = tf.keras.utils.to_categorical(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner_node(graph,sample,n=1):\n",
    "    nodes = graph.nodes\n",
    "    weights = np.concatenate([node.weight for node in nodes])\n",
    "    distance = np.linalg.norm(weights - sample, axis=1)\n",
    "    neuron_ids = np.argsort(distance)\n",
    "    closest_neuron_id = neuron_ids[0:n]\n",
    "    return closest_neuron_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_distance(graph,sample):\n",
    "    nodes = graph.nodes\n",
    "    weights = np.concatenate([node.weight for node in nodes])\n",
    "    distance = np.linalg.norm(weights - sample, axis=1)\n",
    "    neuron_ids = np.argsort(distance)\n",
    "    closest_neuron_id, second_closest_id = neuron_ids[:2]\n",
    "    closest_neuron = nodes[closest_neuron_id]\n",
    "    second_closest = nodes[second_closest_id]\n",
    "    total_error = 0\n",
    "    for to_neuron in list(graph.edges_per_node[closest_neuron]): \n",
    "        edge_id = graph.find_edge_id(to_neuron, closest_neuron)\n",
    "        #print(edge_id)\n",
    "        total_error += distance[graph.edges[edge_id]] \n",
    "    return closest_neuron_id,total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the responsibility of the graph nodes wrt. a certain hypothesis we assign the training labels to the corresponding winner nodes of the training data. This is achieved by fillling a 2D histogram with the winner nodes vs. the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmatches = 2\n",
    "labels  = np.repeat(train_labels,bestmatches)\n",
    "winners = np.arange(0)\n",
    "for sample in train:\n",
    "    winners = np.append(winners, winner_node(model.graph,sample,bestmatches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that winner nodes are peaking at different locations for the various particle types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.repeat(train_labels,bestmatches)\n",
    "histogram, xedges, yedges = np.histogram2d(winners,labels,bins=(len(model.graph.nodes),5))\n",
    "plt.hist2d(winners,labels,bins=(len(model.graph.nodes),5),cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a prediction for a sample feature vector of the test dataset we determine the histogram counts for each particle hypothesis and transform the five values to a softmax probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(graph,sample,histogram):\n",
    "    w = winner_node(graph,sample)[0]  # winner_node returns a list of the first n winners, default is n=1\n",
    "    pred = np.array(histogram[w])  # [el, mu, pi, ka, pr]\n",
    "    pred /= (np.amax(pred) + 1.e-6)\n",
    "    pexp = np.exp(pred)\n",
    "    return pexp / np.sum(pexp)  # softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_dataset.to_numpy()\n",
    "pred_labels = np.arange(0)\n",
    "predictions = np.arange(0)\n",
    "for sample in test:\n",
    "    prediction = predict(model.graph,sample,histogram)\n",
    "    predictions = np.append(predictions, prediction)\n",
    "    label = np.where(prediction == np.amax(prediction))\n",
    "    pred_labels = np.append(pred_labels, label[0][0]) # Take only the first element in case of ambiguities\n",
    "\n",
    "predictions = np.reshape(predictions, (-1, 5))\n",
    "#predictions = to_categorical(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "We evaluate the model performance using the mormalized test data set and compare the output vector to the test labels. The output vector holds the probabilities of the five particle hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#pred_labels = np.argmax(predictions, axis=1)\n",
    "print(confusion_matrix(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, pred_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Physics Control Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the results by plotting the measured data in dependance of the particle momentum. The color index is: protons (yellow), kaons (green), pions (cyan), muons (blue), electrons (red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom = test_dataset[b'mom']\n",
    "svt = test_dataset[b'svt']\n",
    "emc = test_dataset[b'emc']\n",
    "dch = test_dataset[b'dch']\n",
    "drc = test_dataset[b'drc']\n",
    "ifr = test_dataset[b'ifr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(mom, svt, alpha=0.5,\n",
    "            s=5, c=test_labels, cmap='viridis')\n",
    "plt.ylabel('dE/dx SVT')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(mom, svt, alpha=0.5,\n",
    "            s=5, c=pred_labels, cmap='viridis')\n",
    "plt.ylabel('dE/dx SVT')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(mom, emc, alpha=0.5,\n",
    "            s=5, c=test_labels, cmap='viridis')\n",
    "plt.ylabel('Energy [GeV]')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(mom, emc, alpha=0.5,\n",
    "            s=5, c=pred_labels, cmap='viridis')\n",
    "plt.ylabel('Energy [GeV]')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(mom, dch, alpha=0.5,\n",
    "            s=5, c=test_labels, cmap='viridis')\n",
    "plt.ylabel('dch')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(mom, dch, alpha=0.5,\n",
    "            s=5, c=pred_labels, cmap='viridis')\n",
    "plt.ylabel('dch')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(mom, drc, alpha=0.5,\n",
    "            s=5, c=test_labels, cmap='viridis')\n",
    "plt.ylabel('drc')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(mom, drc, alpha=0.5,\n",
    "            s=5, c=pred_labels, cmap='viridis')\n",
    "plt.ylabel('drc')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(mom, ifr, alpha=0.5,\n",
    "            s=10, c=test_labels, cmap='viridis')\n",
    "plt.ylabel('Layers')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(mom, ifr, alpha=0.5,\n",
    "            s=10, c=pred_labels, cmap='viridis')\n",
    "plt.ylabel('Layers')\n",
    "plt.xlabel('momentum [GeV/c]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muon Selection\n",
    "By inspection of the physics control plots it seems difficult to clearly separate muons (blue) from pions (cyan) by simple linear cuts. Thus we want to construct a muon selector based on the network output respecting the probability of the five particle hypotheses. Taking into account the relative a priori probabilities of the particle occurence we can formulate a likelihood ratio to observe a muon (Pions are 5 times more abundant than muons, protons occur at 10% only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electron = predictions[:,0]\n",
    "muon     = predictions[:,1]\n",
    "pion     = predictions[:,2] * 5.0\n",
    "kaon     = predictions[:,3]\n",
    "proton   = predictions[:,4] * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.log(muon) - np.log(pion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot L = log(muon) - log(pion) we are able to clearly separate muons by a cut on L > -2.0 over a wide momentum range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(mom, L, alpha=0.2,\n",
    "            s=10, c=test_labels, cmap='viridis')\n",
    "plt.ylabel('log(muon) - log(pion)')\n",
    "plt.xlabel('momentum [GeV]')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(mom, L, alpha=0.2,\n",
    "            s=10, c=pred_labels, cmap='viridis')\n",
    "plt.ylabel('log(muon) - log(pion)')\n",
    "plt.xlabel('momentum [GeV]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
